{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU-nxGD5wb2m"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRfQaQCcdaUz"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/New_datasets/gnq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDJ7ms8gFTj"
      },
      "source": [
        "#Setting up libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHv0q0eMdmbM"
      },
      "outputs": [],
      "source": [
        "# pip installing all the libraries\n",
        "%%capture\n",
        "!pip install jax\n",
        "!pip install flax\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7kedGU8dpJv"
      },
      "outputs": [],
      "source": [
        "#All the imports\n",
        "from google.colab import output\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import jax\n",
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords\n",
        "import jax.numpy as jnp\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "import torch\n",
        "import torch\n",
        "import urllib3\n",
        "import requests\n",
        "import re\n",
        "import pickle\n",
        "from googlesearch import search\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import timeit\n",
        "import nltk\n",
        "import textwrap\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import FlaxBigBirdForQuestionAnswering, BigBirdTokenizer\n",
        "# import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from urllib.parse import urlparse\n",
        "nltk.download('punkt')\n",
        "from sklearn.cluster import KMeans\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgH8cmkzgKPF"
      },
      "source": [
        "#Utilites and Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_z0QxyKwUIZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def combine_strs(list_of_str):\n",
        "  ans_list = []\n",
        "  i=0\n",
        "  while i < (len(list_of_str)):\n",
        "    j = i\n",
        "    temp_str =\"\"\n",
        "    while(j<len(list_of_str) and len(temp_str.split(\" \"))<256):\n",
        "      temp_str+=list_of_str[j]+\" \"\n",
        "      # print(temp_str)\n",
        "      j+=1\n",
        "    ans_list.append(temp_str)\n",
        "    i=j\n",
        "  return ans_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU-P06VodXOT"
      },
      "outputs": [],
      "source": [
        "#Word Wrap\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8aiczG9f1Nk"
      },
      "outputs": [],
      "source": [
        "#utils\n",
        "class deli:\n",
        "  @staticmethod\n",
        "  def deli_1():\n",
        "    print(\"_______________ this is a delimiter _______________\")\n",
        "  @staticmethod\n",
        "  def deli_2():\n",
        "    print(\"_______________________________________________________________________________________________\")\n",
        "  @staticmethod\n",
        "  def deli_3():\n",
        "    print(\"========================================================================\")\n",
        "def list_to_str(some_list):\n",
        "  return \" \".join(some_list)\n",
        "def split_into_sent(content):\n",
        "  a_list = nltk.tokenize.sent_tokenize(content)\n",
        "  return a_list\n",
        "\n",
        "def harmonic_mean(a):\n",
        "  b=a[1]\n",
        "  a=a[0]\n",
        "  if a==0 or b==0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 2*a*b/(a+b)\n",
        "import re\n",
        "CLEANR = re.compile('<.*?>')\n",
        "spaces = re.compile(' +')\n",
        "# delimiters = re.compile('')\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleantext = re.sub(CLEANR, ' ', raw_html)\n",
        "  cleantext = re.sub(spaces, ' ', cleantext)\n",
        "  return cleantext\n",
        "\n",
        "def simplify(st):\n",
        "  temp = cleanhtml(st)\n",
        "  temp = temp.replace('\\n',' ')\n",
        "  temp = temp.replace('\\t',' ')\n",
        "  return temp\n",
        "\n",
        "\n",
        "def printw(f):\n",
        "  wrapper = textwrap.TextWrapper(width=180)\n",
        "  \n",
        "  word_list = wrapper.wrap(text=f)\n",
        "  \n",
        "# Print each line.\n",
        "  for element in word_list:\n",
        "    print(element)\n",
        "\n",
        "def make_histogram(y):\n",
        "  hist= {}\n",
        "  for i in y:\n",
        "    hist[i] = hist.get(i,0)+1\n",
        "  return hist\n",
        "def combine_strs(list_of_str):\n",
        "  ans_list = []\n",
        "  i=0\n",
        "  while i < (len(list_of_str)):\n",
        "    j = i\n",
        "    temp_str =\"\"\n",
        "    while(j<len(list_of_str) and len(temp_str.split(\" \"))<256):\n",
        "      temp_str+=list_of_str[j]+\" \"\n",
        "      # print(temp_str)\n",
        "      j+=1\n",
        "    ans_list.append(temp_str)\n",
        "    i=j\n",
        "  return ans_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSWzT7QjgPJ9"
      },
      "source": [
        "#Define Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_HD88DMdcDl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "FLAX_MODEL_ID = \"vasudevgupta/flax-bigbird-natural-questions\"\n",
        "model_qa = FlaxBigBirdForQuestionAnswering.from_pretrained(FLAX_MODEL_ID, block_size=64, num_random_blocks=3)\n",
        "tokenizer_qa = BigBirdTokenizer.from_pretrained(FLAX_MODEL_ID)\n",
        "\n",
        "@jax.jit\n",
        "def forward(*args, **kwargs):\n",
        "  return model_qa(*args, **kwargs)\n",
        "\n",
        "def get_answer(question, context):\n",
        "\n",
        "    encoding = tokenizer(question, context, return_tensors=\"jax\", max_length=4096, padding=\"max_length\", truncation=True)\n",
        "    start_scores, end_scores = forward(**encoding).to_tuple()\n",
        "\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
        "\n",
        "    answer_tokens = all_tokens[jnp.argmax(start_scores): jnp.argmax(end_scores)+1]\n",
        "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
        "\n",
        "    return answer\n",
        "def run_code():\n",
        "  question = input(\"Type your question here : \")\n",
        "  context = input(\"Paste the context here : \") \n",
        "  ans = get_answer(question, context)\n",
        "  print(\"Lets see :- \")\n",
        "  print(ans)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz3bc1IGfw1P"
      },
      "outputs": [],
      "source": [
        "def get_answer_1(question, context):\n",
        "  # print(\"I am in this function\")\n",
        "  if(context==\" this is a meaningless content due to repetition and timeout as well\"):\n",
        "    return (\"No Answer due to meaningless content due to repetition and timeout as well\",(0,0))\n",
        "\n",
        "  encoding = tokenizer_qa(question, context, return_tensors=\"jax\", max_length=4096, padding=\"max_length\", truncation=False)\n",
        "  start_scores, end_scores = forward(**encoding).to_tuple()\n",
        "  # print(start_scores.shape, end_scores)\n",
        "  # Let's take the most likely token using `argmax` and retrieve the answer\n",
        "  all_tokens = tokenizer_qa.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
        "  # print(all_tokens)\n",
        "  g = (jnp.max(start_scores), jnp.max(end_scores))\n",
        "  start_token = jnp.argmax(start_scores)\n",
        "  end_token =  jnp.argmax(end_scores)\n",
        "  answer_tokens = all_tokens[start_token: end_token+1]\n",
        "  answer = tokenizer_qa.decode(tokenizer_qa.convert_tokens_to_ids(answer_tokens))\n",
        "  answer=answer.replace(\"[SEP]\", \"\")\n",
        "  answer=answer.replace(\"[CLS]\", \"\")\n",
        "  # print(start_token, end_token)\n",
        "  if(answer==\"\"):\n",
        "    # print(\"Stuck here\")\n",
        "    # print(end_scores.shape)\n",
        "    end_token_ = jax.numpy.argsort((end_scores[0]))\n",
        "    # print(end_token_, end_scores, \"Sanity Check\")\n",
        "    # pprint(list(end_scores[0]))\n",
        "    if(len(end_token_)==1):\n",
        "      answer = \"No Meaningful Answer due to lack of content only 1 word\"\n",
        "    else:\n",
        "      # print(\"I am here\")\n",
        "      end_token = end_token_[-2]\n",
        "      g = (g[0],end_scores[0][end_token_[-2]])\n",
        "      answer_tokens = all_tokens[start_token: end_token+1]\n",
        "      answer = tokenizer_qa.decode(tokenizer_qa.convert_tokens_to_ids(answer_tokens))\n",
        "      answer=answer.replace(\"[SEP]\", \"\")\n",
        "      answer=answer.replace(\"[CLS]\", \"\")\n",
        "      \n",
        "    # print(g, ((start_scores), (end_scores)))\n",
        "  answer=answer.replace(\"⁇\", \"\")\n",
        "  if(answer ==\"\" or answer ==\" \"):\n",
        "    g = (1 if g[0]==0 else g[0],0)\n",
        "  # print(\"I am leaving this function\")\n",
        "  return (answer,g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dDgp2IcgStd"
      },
      "source": [
        "#Crawler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0zL-35Eee9M"
      },
      "outputs": [],
      "source": [
        "def google_search(q, n, ls):\n",
        "  for j in search(q, tld=\"com\", num=10, stop=10, pause=2):\n",
        "      ls.append(j)\n",
        "\n",
        "\n",
        "def scraper(query_list,n_links=10):\n",
        "  df = pd.DataFrame()\n",
        "  count_F=0\n",
        "  for i,q in enumerate(query_list):\n",
        "    print(\"i->\", i)\n",
        "    row =dict()\n",
        "    q = q[0]\n",
        "    row['query'] = q\n",
        "    ls = []\n",
        "    unique_url = set()\n",
        "    print(q[0])\n",
        "    google_search( q, n_links ,ls)\n",
        "    # print(ls)\n",
        "    for j,url in enumerate(ls):\n",
        "      print(\"\\tj->\", j)\n",
        "      # webpage = requests.get(url)\n",
        "      # webpage= webpage.content.encode('utf-8')\n",
        "      domain = urlparse(url).netloc\n",
        "      if domain in unique_url:\n",
        "        print(\"same domain explored before\")\n",
        "        pass\n",
        "      unique_url.add(domain)\n",
        "      try:\n",
        "          http_pool = urllib3.connection_from_url(url)\n",
        "          r = http_pool.urlopen('GET',url)\n",
        "          content= r.data.decode('utf-8')\n",
        "\n",
        "      except:\n",
        "          try:\n",
        "              webp = requests.get(url)\n",
        "              content = webp.content.encode('utf-8')\n",
        "          except:\n",
        "              content= \"This is a meaningless content   \"\n",
        "              print(\"F\")\n",
        "              count_F+=1\n",
        "              pass\n",
        "\n",
        "      \n",
        "      \n",
        "      soup_obj = BeautifulSoup(content, 'lxml')\n",
        "\n",
        "      for script in soup_obj([\"script\", \"style\"]):\n",
        "          script.extract()\n",
        "\n",
        "      p1  = []\n",
        "      for para in soup_obj.find_all([\"p\",\"li\"]):\n",
        "        p1.append(para.get_text()+\" \")\n",
        "      row[str(j)+\"_Content\"] = \"\\n\".join(p1)\n",
        "      row[str(j)+\"_Link\"] = url\n",
        "    df = df.append(row, ignore_index=True)\n",
        "  print(count_F)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfVPAD9-B8O7"
      },
      "outputs": [],
      "source": [
        "#google search api + \n",
        "def scraper(query_list,n_links=10):\n",
        "  df = pd.DataFrame()\n",
        "  for i,q in enumerate(query_list):\n",
        "    print(\"i\", i)\n",
        "    row =dict()\n",
        "    row['query'] = q[0] \n",
        "    ls = []\n",
        "    \n",
        "    google_search(q,n_links, ls)\n",
        "    for j,url in enumerate(ls):\n",
        "      print(\"\\tj\", j)\n",
        "      webpage = requests.get(url)\n",
        "      # webpage= webpage.content.encode('utf-8')\n",
        "      if (i,j) in [(0,8)]:\n",
        "        try:\n",
        "          decoded_webpage = webpage.content.decode('utf-8')\n",
        "        except:\n",
        "          print(i,j,\"checked cases\")\n",
        "          pass\n",
        "      else:\n",
        "        decoded_webpage = webpage.content.decode('utf-8')\n",
        "\n",
        "      soup_obj = BeautifulSoup(decoded_webpage, 'lxml')\n",
        "\n",
        "      for script in soup_obj([\"script\", \"style\"]):\n",
        "          script.extract()   \n",
        "\n",
        "      p1  = []\n",
        "      for para in soup_obj.find_all([\"p\",\"li\"]):\n",
        "        p1.append(para.get_text()+\" \")\n",
        "      row[str(j)+\"_Content\"] = \"\\n\".join(p1)\n",
        "      row[str(j)+\"_Link\"] = url\n",
        "    df = df.append(row, ignore_index=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35OCjRDanVym"
      },
      "source": [
        "#Document Shortening Algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPG_jUINl2e1"
      },
      "source": [
        "##TFIDF + Cosine similarity + window size =2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlpUG6cFnSXf"
      },
      "outputs": [],
      "source": [
        "#TFIDF + Cosine similarity + window size =2\n",
        "# def select_sent(query,sent_list,thresh=0.2):\n",
        "#   vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
        "#   try:\n",
        "#     tfidf = vect.fit(sent_list)\n",
        "#   except:\n",
        "#     return [(i,k) for i,k in enumerate(sent_list)]\n",
        "\n",
        "#   ans=[]\n",
        "#   for i,sent in enumerate(sent_list):\n",
        "#     s = [query, sent]\n",
        "#     tfidf_1 = tfidf.transform(s)\n",
        "#     pairwise_similarity = tfidf_1 * tfidf_1.T\n",
        "#     simma = ( pairwise_similarity.toarray())[0,1]\n",
        "#     if(simma>thresh):\n",
        "#       ans.append((i,sent))\n",
        "#   return ans\n",
        "  \n",
        "# def list2para(ans_list,sent_list,window_size=2):\n",
        "  \n",
        "#   d= set()\n",
        "#   len_sent= len(sent_list)\n",
        "#   for i,k in ((ans_list)):\n",
        "#     for j in range(-window_size,window_size+1):\n",
        "#       if(i+j>=0 and i+j<len_sent):\n",
        "#         d.add((i+j,sent_list[i+j]))\n",
        "#   d = list(d)\n",
        "#   d =sorted(d)\n",
        "#   return list_to_str([j[1] for j in list(d)])\n",
        "\n",
        "\n",
        "# def answer_2(df):\n",
        "#   df=df.fillna(\"\")\n",
        "#   list_1=[]\n",
        "#   for i, k in df.iterrows():\n",
        "#     q = k['query']\n",
        "#     print(i)\n",
        "#     list_temp=[]\n",
        "#     for j in range( 10):\n",
        "#       c = k[str(j)+\"_Content\"]\n",
        "#       # c = make_tuple()\n",
        "#       c = simplify(c)\n",
        "\n",
        "\n",
        "#       sent_list = split_into_sent(c)\n",
        "      \n",
        "#       ans_list = select_sent(q,sent_list,thresh= 0.15)\n",
        "\n",
        "#       l2p = list2para(ans_list,sent_list)\n",
        "#       ans= get_answer_1(q,l2p)\n",
        "#       list_temp.append((harmonic_mean(ans[1]),ans[0],j))\n",
        "#     list_temp_1 = sorted(list_temp)\n",
        "#     list_temp_1.reverse()\n",
        "#     list_temp_temp = [(ii[1],ii[2]) for ii in list_temp_1]\n",
        "#     list_temp = [q,k['ans']]\n",
        "#     list_temp.extend(list_temp_temp)\n",
        "#     list_temp=list_temp[:7]\n",
        "#     list_1.append(list_temp)\n",
        "#   return list_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmVs80UbK2A-"
      },
      "source": [
        "##Passage Ranking with MS Marco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sus5uHiEK12x"
      },
      "outputs": [],
      "source": [
        "model_n='cross-encoder/ms-marco-TinyBERT-L-2'\n",
        "model_pr = AutoModelForSequenceClassification.from_pretrained(model_n)\n",
        "tokenizer_pr = AutoTokenizer.from_pretrained(model_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkbCsccQpaZX"
      },
      "outputs": [],
      "source": [
        "def score_for_one(q, list_of_passages):\n",
        "  \n",
        "  features=tokenizer_pr([q],list_of_passages,padding =True, truncation = True, return_tensors='pt')\n",
        "\n",
        "  model_pr.eval()\n",
        "  with torch.no_grad():\n",
        "      scores = model_pr(**features).logits\n",
        "      return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQEZYPuUt1aw"
      },
      "outputs": [],
      "source": [
        "def rank_passages_1(df_c,top_k=9):\n",
        "  df_c=df_c.fillna(\"\")\n",
        "  df_ans = pd.DataFrame()\n",
        "  len_array=[]\n",
        "  for i, k in df_c.iterrows():\n",
        "    row_dict={}\n",
        "    for j in range(10):\n",
        "      print(i,j)\n",
        "      passage_ranker=[]\n",
        "      q=df_c['query'].iloc[i]\n",
        "\n",
        "      content= df_c[str(j) +\"_Content\"].iloc[i]\n",
        "      printw(\"Question \"+str(i) +\":\" + q)\n",
        "      printw(\"Content \"+str(j) +\":\" + df_c[str(j) +\"_Link\"].iloc[i])\n",
        "      if(content==\"\"):\n",
        "        row_dict[str(j)] = \"This is Empty Content\"\n",
        "        continue\n",
        "      cont_l= content.split(\"\\n\")\n",
        "      cont_l_1=[]\n",
        "      for ii in cont_l:\n",
        "        if ii == \"\":\n",
        "          continue\n",
        "        if len(ii.split(\" \"))>450:\n",
        "          print(\"I am a problem, u shud see me\", i,j)\n",
        "        \n",
        "        cont_l_1.append(ii)\n",
        "      # cont_l_2 = combine_strs(cont_l_1)\n",
        "      # cont_l_1=cont_l_2\n",
        "      print(len(cont_l_1))\n",
        "      len_array_1 = [len(i.split(\" \")) for i in cont_l_1]\n",
        "      len_array.extend(len_array_1)\n",
        "\n",
        "      scores = []\n",
        "      \n",
        "      row_dict['query']=q\n",
        "      for c,ii in enumerate(cont_l_1):\n",
        "        score = score_for_one(q, [ii])\n",
        "        scores.append((score, c))\n",
        "      scores = sorted(scores,reverse=True)\n",
        "      candidates = scores[:top_k]\n",
        "      print([i[0] for i in candidates])\n",
        "      candidates = [(j,i) for i,j in candidates]\n",
        "      candidates = sorted(candidates)\n",
        "      if(candidates[0][0] <-4.8):\n",
        "        row_dict[str(j)] = \"This is Useless Content\"\n",
        "        continue\n",
        "    \n",
        "      candidates=[cont_l_1[i[0]] for i in candidates]\n",
        "      # candidates=[candidates[i[1]] for i in candidates]\n",
        "      row_dict[str(j)] = \" \".join(candidates)\n",
        "      deli.deli_2()\n",
        "      deli.deli_2()\n",
        "      printw(\"Selected Content \"+str(j)+ \": \"+\" \".join(candidates) )\n",
        "      # row_dict[str(j)] = \" \".join(candidates)\n",
        "      deli.deli_2()\n",
        "      input()\n",
        "      output.clear()\n",
        "      \n",
        "    df_ans  = df_ans.append(row_dict,ignore_index=True)\n",
        "  print(make_histogram(len_array))\n",
        "  return df_ans\n",
        "\n",
        "def rank_sentences(q, passage, n_cls=7, select_cls=4):\n",
        "  sent_l = nltk.sent_tokenize(passage)\n",
        "  print(\"Total sentences:\",len(sent_l))\n",
        "  if(len(sent_l)==0 or (len(sent_l)==1 and sent_l[0]=='')):\n",
        "    return \"Insufficient String\"\n",
        "  if(len(sent_l)<n_cls): \n",
        "    return \" \".join(passage)\n",
        "  scores = []\n",
        "  for i, k  in enumerate(sent_l):\n",
        "    score = score_for_one(q,[k])\n",
        "    scores.append((score,i))\n",
        "  scores = sorted(scores,reverse=True)\n",
        "  # print(scores)\n",
        "  scoo = [i[0] for i in scores]\n",
        "  x= np.array(scoo)\n",
        "  km = KMeans(n_clusters=n_cls, random_state=42, max_iter=500)\n",
        "  km.fit(x.reshape(-1,1))\n",
        "  y = km.labels_\n",
        "  sent_ll = [sent_l[iiii] for jjjj,iiii in scores]\n",
        "  # print(list(zip(sent_ll,y)))\n",
        "  candidates= []\n",
        "  true_label = select_cls\n",
        "  selecting = set()\n",
        "  for ii,iii in enumerate(y):\n",
        "    if(len(selecting)<select_cls):\n",
        "      selecting.add(iii)\n",
        "    if iii in selecting:\n",
        "      candidates.append((scoo[ii], scores[ii][1]))\n",
        "  print(\"Selected Sentences Count:\",len(candidates))\n",
        "  candidates = [(j,i) for i,j in candidates]\n",
        "  candidates = sorted(candidates)  \n",
        "  candidates=[sent_l[i[0]] for i in candidates]\n",
        "  return \" \".join(candidates)\n",
        "\n",
        "\n",
        "def rank_passages_2(df_c,top_k=9):\n",
        "  df_c=df_c.fillna(\"\")\n",
        "  df_ans = pd.DataFrame()\n",
        "  # len_array=[]\n",
        "  for i, k in df_c.iterrows():\n",
        "    row_dict={}\n",
        "    for j in range(10):\n",
        "      print(\"Index\",i,j)\n",
        "      passage_ranker=[]\n",
        "      q=df_c['query'].iloc[i]\n",
        "\n",
        "      content= df_c[str(j) +\"_Content\"].iloc[i]\n",
        "      # printw(\"Question \"+str(i) +\":\" + q)\n",
        "      # printw(\"Content \"+str(j) +\":\" + df_c[str(j) +\"_Link\"].iloc[i])\n",
        "      if(content==\" this is a meaningless content due to repetition and timeout as well\"):\n",
        "        row_dict[str(j)] =\" this is a meaningless content due to repetition and timeout as well\"\n",
        "        continue\n",
        "      start = timeit.timeit()\n",
        "      if i == 58 and j ==6:\n",
        "        row_dict[str(j)] = \" this is a meaningless content due to repetition and timeout as well\"\n",
        "        continue\n",
        "      row_dict[str(j)] = rank_sentences(q,content)\n",
        "      end = timeit.timeit()\n",
        "      print(end-start)\n",
        "      \n",
        "    df_ans  = df_ans.append(row_dict,ignore_index=True)\n",
        "  # print(make_histogram(len_array))\n",
        "  return df_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAqRcRpLyxXO"
      },
      "outputs": [],
      "source": [
        "cont = '''\n",
        "\n",
        "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\n",
        "\n",
        "The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\n",
        "\n",
        "The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n",
        "\n",
        "The term \"k-means\" was first used by James MacQueen in 1967,[2] though the idea goes back to Hugo Steinhaus in 1956.[3] The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982.[4] In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.[5]\n",
        "\n",
        "Commonly used initialization methods are Forgy and Random Partition.[10] The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al.,[10] the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al.,[11] however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach[12] performs \"consistently\" in \"the best group\" and k-means++ performs \"generally well\".\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "query = \"Is Kmeans computationally efficient?\"\n",
        "deli.deli_2()\n",
        "ans = rank_sentences(query, cont, n_cls=7, select_cls =4)\n",
        "print(ans)\n",
        "print(get_answer_1(query,ans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScITJlPe8GV9"
      },
      "outputs": [],
      "source": [
        "def rank_pages(df_c):\n",
        "  df_c= df_c.fillna(\"Insufficient String\")\n",
        "  list_1= []\n",
        "  for i, k in df_c.iterrows():\n",
        "    print(i)\n",
        "    q = df_a['query'].iloc[i]\n",
        "    temp_list = []\n",
        "    for j in range(10):\n",
        "      print(\"\\t\",j)\n",
        "      try:\n",
        "        c = k[str(j)]\n",
        "        \n",
        "        # print((c))\n",
        "        try:\n",
        "          # print(\"I am here\")\n",
        "          if(c == 'Insufficient String' or c ==' ' or c=='' ):\n",
        "            continue\n",
        "          print(\"\\t\\t\",len(nltk.sent_tokenize(c)))\n",
        "          ans, scores = get_answer_1(q,c)\n",
        "          \n",
        "          temp_list.append((harmonic_mean(scores), j, ans))\n",
        "        except Exception as eeee:\n",
        "\n",
        "          print(\"Error at stage 1:\",eeee)\n",
        "\n",
        "          c = rank_pass(q,c)\n",
        "          print(\"\\t\\t After I have changed\",len(nltk.sent_tokenize(c)))\n",
        "          \n",
        "          ans,scores = get_answer_1(q,c)\n",
        "          temp_list.append((harmonic_mean(scores), j, ans))\n",
        "      except:\n",
        "        print(\"skipped bcz it took time\")\n",
        "    temp_list = sorted(temp_list, reverse=True)\n",
        "    temp_list= temp_list[:3]\n",
        "    # print(temp_list)\n",
        "    temp_list = [ ii[2] for ii in temp_list]\n",
        "    list_temp =[q]\n",
        "    list_temp.extend(temp_list)\n",
        "    list_1.append(list_temp)\n",
        "  return list_1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DPR\n"
      ],
      "metadata": {
        "id": "ireUahK55wCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO6eMBh053KI"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiWvM2hU53KK"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "model_dpr = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPh5wrFQ53KK"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHX_-qEx53KK"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def get_dpr_similarity(ques, cont):\n",
        "  \n",
        "  ctx_tokens = ctx_tokenizer(cont, max_length = 512, truncation = True, return_tensors='pt')[\"input_ids\"]\n",
        "  q_tokens = q_tokenizer(ques,return_tensors='pt')[\"input_ids\"]\n",
        "  ctx_encoded = ctx_encoder(ctx_tokens).pooler_output\n",
        "  q_encoded = q_encoder(q_tokens).pooler_output\n",
        "  similarity = (q_encoded@(ctx_encoded.T)).item()\n",
        "\n",
        "  return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmXr1jYN53KK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def combine_strs(list_of_str):\n",
        "  ans_list = []\n",
        "  i=0\n",
        "  while i < (len(list_of_str)):\n",
        "    j = i\n",
        "    temp_str =\"\"\n",
        "    while(j<len(list_of_str) and len(temp_str.split(\" \"))<256):\n",
        "      temp_str+=list_of_str[j]+\" \"\n",
        "      # print(temp_str)\n",
        "      j+=1\n",
        "    ans_list.append(temp_str)\n",
        "    i=j\n",
        "  return ans_list\n",
        "\n",
        "def rank_pass(q,ct,alpha=0.5):\n",
        "  cl = nltk.sent_tokenize(ct)\n",
        "  \n",
        "  if len(cl) > 3000:\n",
        "    cl = combine_strs(cl)\n",
        "\n",
        "  print(len(cl))\n",
        "  similarity_scores = [get_dpr_similarity(q,a) for i,a in enumerate(cl)]\n",
        "  # similarity_scores = sorted(similarity_scores, reverse = True)\n",
        "\n",
        "  similarity_scores = np.array(similarity_scores)\n",
        "  # print(similarity_scores)\n",
        "  mean = np.mean(similarity_scores)\n",
        "  std = np.std(similarity_scores) * alpha\n",
        "  thresh1 = mean+std\n",
        "  # s1 = np.where(similarity_scores >= thresh2,1,0)\n",
        "  s2 = np.where(similarity_scores >= thresh1,1,0)\n",
        "  s = np.where(s2 == 1)[0]\n",
        "  # print(s)\n",
        "  cl_list = [cl[j] for j in s]\n",
        "\n",
        "  return \" \".join(cl_list)\n",
        "\n",
        "def rank_passages_3(df_c,top_k=9):\n",
        "  df_c=df_c.fillna(\"\")\n",
        "  df_ans = pd.DataFrame()\n",
        "  # len_array=[]\n",
        "  for i, k in df_c.iterrows():\n",
        "    row_dict={}\n",
        "    for j in range(10):\n",
        "      print(\"Index\",i,j)\n",
        "      passage_ranker=[]\n",
        "      q=df_c['query'].iloc[i]\n",
        "\n",
        "      content= df_c[str(j) +\"_Content\"].iloc[i]\n",
        "      # printw(\"Question \"+str(i) +\":\" + q)\n",
        "      # printw(\"Content \"+str(j) +\":\" + df_c[str(j) +\"_Link\"].iloc[i])\n",
        "      if(content==\" this is a meaningless content due to repetition and timeout as well\"):\n",
        "        row_dict[str(j)] =\" this is a meaningless content due to repetition and timeout as well\"\n",
        "        continue\n",
        "      start = timeit.timeit()\n",
        "      row_dict[str(j)] = rank_pass(q,content)\n",
        "      end = timeit.timeit()\n",
        "      print(end-start)\n",
        "      \n",
        "    df_ans  = df_ans.append(row_dict,ignore_index=True)\n",
        "  # print(make_histogram(len_array))\n",
        "  return df_ans"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}